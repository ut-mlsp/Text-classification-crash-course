{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nc_fgf8lMIWv"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing import text\n",
    "from tensorflow.keras.layers import Embedding, Dropout, SeparableConv1D, MaxPooling1D, GlobalAveragePooling1D, Dense, Input, Conv1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xx2VzuoaMbdA"
   },
   "outputs": [],
   "source": [
    "def sepr(elem, start=1):\n",
    "    latin = string.ascii_lowercase + string.digits + '$'\n",
    "    for ind in range(start, len(elem)):\n",
    "        if elem[ind] == ' ':\n",
    "            continue\n",
    "        a = elem[ind-1] in latin\n",
    "        b = elem[ind] in latin\n",
    "        if a != b:\n",
    "            return sepr(elem[:ind] + ' ' + elem[ind:], ind + 2)\n",
    "    return elem\n",
    "\n",
    "def sequence_vectorize(train_texts, val_texts, TOP_K=20000, MAX_SEQUENCE_LENGTH=500):\n",
    "    \"\"\"Vectorizes texts as sequence vectors.\n",
    "\n",
    "    1 text = 1 sequence vector with fixed length.\n",
    "\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        val_texts: list, validation text strings.\n",
    "\n",
    "    # Returns\n",
    "        x_train, x_val, word_index: vectorized training and validation\n",
    "            texts and word index dictionary.\n",
    "    \"\"\"\n",
    "    # Create vocabulary with training texts.\n",
    "    tokenizer = text.Tokenizer(num_words=TOP_K)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "    # Vectorize training and validation texts.\n",
    "    x_train = tokenizer.texts_to_sequences(train_texts)\n",
    "    x_val = tokenizer.texts_to_sequences(val_texts)\n",
    "\n",
    "    # Get max sequence length.\n",
    "    max_length = len(max(x_train, key=len))\n",
    "    if max_length > MAX_SEQUENCE_LENGTH:\n",
    "        max_length = MAX_SEQUENCE_LENGTH\n",
    "\n",
    "    # Fix sequence length to max value. Sequences shorter than the length are\n",
    "    # padded in the beginning and sequences longer are truncated\n",
    "    # at the beginning.\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "    x_val = sequence.pad_sequences(x_val, maxlen=max_length)\n",
    "    return x_train, x_val, tokenizer.word_index\n",
    "    \n",
    "    \n",
    "def _data_generator(x_t, x_d, y, batch_size):\n",
    "    \"\"\"Generates batches of vectorized texts for training/validation.\n",
    "\n",
    "    # Arguments\n",
    "        x_t: np.matrix, feature(title) matrix.\n",
    "        x_d: np.matrix, feature(description) matrix.\n",
    "        y: np.ndarray, labels.\n",
    "        batch_size: int, number of samples per batch.\n",
    "\n",
    "    # Returns\n",
    "        Yields feature and label data in batches.\n",
    "    \"\"\"\n",
    "    num_samples = x_t.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "    if num_samples % batch_size:\n",
    "        num_batches += 1\n",
    "\n",
    "    while 1:\n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = (i + 1) * batch_size\n",
    "            if end_idx > num_samples:\n",
    "                end_idx = num_samples\n",
    "            x_t_batch = x_t[start_idx:end_idx]\n",
    "            x_d_batch = x_d[start_idx:end_idx]\n",
    "            y_batch = y[start_idx:end_idx]\n",
    "            yield [x_t_batch, x_d_batch], y_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eNNe1s4YMdMN"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('divar_posts_dataset.csv', \n",
    "usecols=['cat1', 'cat2', 'cat3', 'title', 'desc'])\n",
    "data['cat1'] = data['cat1'].fillna('na')\n",
    "data['cat2'] = data['cat2'].fillna('na')\n",
    "data['cat3'] = data['cat3'].fillna('na')\n",
    "data['desc'] = data['desc'].fillna('')\n",
    "data['title'] = data['title'].fillna('')\n",
    "data['cats'] = data.cat1 + '_' + data.cat2 + '_' + data.cat3\n",
    "data = data[np.bitwise_and(data.cats != 'na_mobile_200000', np.bitwise_and(data.cats != 'na_mobile_270000', data.cats != 'na_mobile_8000'))]\n",
    "data.desc = data.desc.map(sepr)\n",
    "data.title = data.title.map(sepr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ibF11cNVQ8Kq"
   },
   "outputs": [],
   "source": [
    "X_desc_train, X_desc_val, X_title_train, X_title_val, y_train, y_val = train_test_split(data.desc, data.title, data.cats, test_size=0.2, random_state=63)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train.values)\n",
    "y_val = le.transform(y_val.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GPoKN-hUQ-57"
   },
   "outputs": [],
   "source": [
    "X_desc_train, X_desc_val, word_index_desc = sequence_vectorize(X_desc_train, X_desc_val, TOP_K=55000)\n",
    "X_title_train, X_title_val, word_index_title = sequence_vectorize(X_title_train, X_title_val, TOP_K=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "MRzqJQ1PRZWX",
    "outputId": "6d63943e-2019-4ebc-8810-c797d4775507"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(796405, 322) (199102, 322)\n",
      "(796405, 19) (199102, 19)\n",
      "(796405,) (199102,)\n"
     ]
    }
   ],
   "source": [
    "print(X_desc_train.shape, X_desc_val.shape)\n",
    "print(X_title_train.shape, X_title_val.shape)\n",
    "print(y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "colab_type": "code",
    "id": "ZYofRdh0Rqt5",
    "outputId": "4439be0c-cd03-4531-b1fd-f3281f987731"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "num_features_title = min(len(word_index_title) + 1, 20000)\n",
    "num_features_desc = min(len(word_index_desc) + 1, 55000)\n",
    "embedding_dim_title = 50\n",
    "embedding_dim_desc = 100\n",
    "kernel_size = 3\n",
    "filters = 32\n",
    "pool_size = 3\n",
    "dropout_rate = 0.5\n",
    "num_classes = max(y_train) + 1\n",
    "\n",
    "ts_input = Input(shape=(X_title_train.shape[1],))\n",
    "ts_model = ts_input\n",
    "ts_model = Embedding(input_dim=num_features_title,\n",
    "                            output_dim=embedding_dim_title,\n",
    "                            input_length=(X_title_train.shape[1],))(ts_model)\n",
    "ts_model = Dropout(rate=dropout_rate)(ts_model)\n",
    "ts_model = Conv1D(filters=filters,\n",
    "                          kernel_size=kernel_size,\n",
    "                          activation='relu',\n",
    "                          bias_initializer='random_uniform',\n",
    "                          padding='same')(ts_model)\n",
    "ts_model = Conv1D(filters=filters,\n",
    "                          kernel_size=kernel_size,\n",
    "                          activation='relu',\n",
    "                          bias_initializer='random_uniform',\n",
    "                          padding='same')(ts_model)\n",
    "ts_model = MaxPooling1D(pool_size=pool_size)(ts_model)\n",
    "ts_model = Conv1D(filters=filters * 2,\n",
    "                          kernel_size=kernel_size,\n",
    "                          activation='relu',\n",
    "                          bias_initializer='random_uniform',\n",
    "                          padding='same')(ts_model)\n",
    "ts_model = Conv1D(filters=filters * 2,\n",
    "                          kernel_size=kernel_size,\n",
    "                          activation='relu',\n",
    "                          bias_initializer='random_uniform',\n",
    "                          padding='same')(ts_model)\n",
    "ts_model = GlobalAveragePooling1D()(ts_model)\n",
    "\n",
    "ds_input = Input(shape=(X_desc_train.shape[1],))\n",
    "ds_model = ds_input\n",
    "ds_model = Embedding(input_dim=num_features_desc,\n",
    "                            output_dim=embedding_dim_desc,\n",
    "                            input_length=(X_desc_train.shape[1],))(ds_model)\n",
    "ds_model = Dropout(rate=dropout_rate)(ds_model)\n",
    "ds_model = Conv1D(filters=filters,\n",
    "                          kernel_size=kernel_size,\n",
    "                          activation='relu',\n",
    "                          bias_initializer='random_uniform',\n",
    "                          padding='same')(ds_model)\n",
    "ds_model = Conv1D(filters=filters,\n",
    "                          kernel_size=kernel_size,\n",
    "                          activation='relu',\n",
    "                          bias_initializer='random_uniform',\n",
    "                          padding='same')(ds_model)\n",
    "ds_model = MaxPooling1D(pool_size=pool_size)(ds_model)\n",
    "ds_model = Conv1D(filters=filters * 2,\n",
    "                          kernel_size=kernel_size,\n",
    "                          activation='relu',\n",
    "                          bias_initializer='random_uniform',\n",
    "                          padding='same')(ds_model)\n",
    "ds_model = Conv1D(filters=filters * 2,\n",
    "                          kernel_size=kernel_size,\n",
    "                          activation='relu',\n",
    "                          bias_initializer='random_uniform',\n",
    "                          padding='same')(ds_model)\n",
    "ds_model = GlobalAveragePooling1D()(ds_model)\n",
    "\n",
    "merged = tf.keras.layers.Concatenate()([ts_model, ds_model])\n",
    "merged = Dropout(rate=dropout_rate)(merged)\n",
    "merged = Dense(num_classes, activation='softmax')(merged)\n",
    "newModel = tf.keras.models.Model([ts_input,ds_input], merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cUklgd0cSe4f"
   },
   "outputs": [],
   "source": [
    "learning_rate=1e-3\n",
    "optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "newModel.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "batch_size = 64\n",
    "# Alaki :)\n",
    "num_features = min(len(word_index_desc) + 1, 20000)\n",
    "training_generator = _data_generator(\n",
    "    X_title_train, X_desc_train, y_train, batch_size)\n",
    "validation_generator = _data_generator(\n",
    "    X_title_val, X_desc_val, y_val, batch_size)\n",
    "\n",
    "# Get number of training steps. This indicated the number of steps it takes\n",
    "# to cover all samples in one epoch.\n",
    "steps_per_epoch = X_title_train.shape[0] // batch_size\n",
    "if X_title_train.shape[0] % batch_size:\n",
    "    steps_per_epoch += 1\n",
    "\n",
    "# Get number of validation steps.\n",
    "validation_steps = X_title_train.shape[0] // batch_size\n",
    "if X_title_train.shape[0] % batch_size:\n",
    "    validation_steps += 1\n",
    "    \n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=6), \n",
    "        tf.keras.callbacks.ModelCheckpoint('output/sepCNN_total.hdf5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "hlUvm5j2Syvi",
    "outputId": "883410e7-ca68-4e51-aba8-09035f98aef0"
   },
   "outputs": [],
   "source": [
    "history = newModel.fit_generator(\n",
    "            generator=training_generator,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=validation_steps,\n",
    "            callbacks=callbacks,\n",
    "            epochs=1000,\n",
    "            verbose=1,\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "CNN_refactor.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
